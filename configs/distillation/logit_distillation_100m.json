{
  "distillation_type": "logit",
  "alpha": 0.5,
  "temperature": 2.0,
  "teacher_model_name": "codellama/CodeLlama-7b-hf",
  "teacher_model_device": "cuda",
  "teacher_dtype": "float16",
  "student_config_path": "configs/model/100m_laptop.json",
  "train_data_path": "data/distillation/train.jsonl",
  "val_data_path": null,
  "batch_size": 4,
  "gradient_accumulation_steps": 8,
  "learning_rate": 0.0001,
  "warmup_steps": 1000,
  "max_steps": 50000,
  "max_epochs": null,
  "weight_decay": 0.01,
  "max_grad_norm": 1.0,
  "max_seq_length": 2048,
  "use_gradient_checkpointing": true,
  "use_mixed_precision": true,
  "mixed_precision_dtype": "bf16",
  "logging_steps": 10,
  "eval_steps": 500,
  "save_steps": 1000,
  "output_dir": "checkpoints/distilled_100m_logit",
  "use_wandb": false,
  "wandb_project": "demiurgic-distillation",
  "wandb_run_name": "100m-logit-codellama7b",
  "save_total_limit": 3,
  "resume_from_checkpoint": null,
  "device": "cuda",
  "num_workers": 4,
  "seed": 42
}
