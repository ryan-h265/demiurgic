javascript
```javascript
import fetch from 'node-fetch';
import { load } from 'cheerio';

// Fetch a page and extract all anchor hrefs
export async function scrapeLinks(url) {
  const resp = await fetch(url);
  if (!resp.ok) throw new Error(`Request failed: ${resp.status}`);
  const html = await resp.text();
  const $ = load(html);
  const links = [];
  $('a[href]').each((_, el) => {
    links.push($(el).attr('href'));
  });
  return links;
}
```
**Explanation:** Use `fetch` to download the HTML, parse it with Cheerio (a server-side jQuery-like library), then collect `href` attributes from all anchor tags into an array.
**Complexity:** Time: O(page_size + links); Space: O(page_size) for the HTML and DOM representation.
**Example:**
Input: scrapeLinks('https://example.com')
Output: ['https://www.iana.org/domains/example']
**Notes:**
- Requires `node-fetch` and `cheerio` packages; install with `npm install node-fetch cheerio`.
- Respect robots.txt and site terms when scraping.
- Add timeouts/retries to harden against network issues.
