# Training with Knowledge Distillation

This guide explains how to train your student model using the generated distillation data.

## Three Distillation Approaches

### 1. Output Distillation (Simplest)

**What it is:** Train your model on the text responses generated by the teacher (GPT-4, Claude, etc.)

**Pros:**
- ✅ No teacher model needed during training (just the data)
- ✅ Works on laptop/limited hardware
- ✅ Fast training
- ✅ No GPU memory needed for teacher

**Cons:**
- ❌ Only learns from final text, not the reasoning process
- ❌ Miss out on probability distributions

**Best for:** Laptop training, limited hardware, getting started quickly

**Example:**
```bash
python scripts/train_with_distillation.py \
    --distillation-type output \
    --train-data data/distillation/train.jsonl \
    --student-config configs/model/100m_laptop.json \
    --output-dir checkpoints/distilled_100m
```

### 2. Logit Distillation (Most Powerful)

**What it is:** Train using the teacher's probability distributions (soft labels), not just the final text

**How it works:**
- Teacher model runs alongside student during training
- Student learns to match teacher's probability distributions
- Uses KL divergence loss with temperature scaling
- Transfers "dark knowledge" - the uncertainties and relationships

**Pros:**
- ✅ Learn richer information than just text
- ✅ Captures teacher's "uncertainty" and reasoning
- ✅ Often better final performance

**Cons:**
- ❌ Requires teacher model loaded during training
- ❌ 2x GPU memory (student + teacher)
- ❌ Slower training (2 forward passes per batch)
- ❌ Need GPU for teacher inference

**Best for:** Cloud training with GPU, maximum quality

**Example:**
```bash
python scripts/train_with_distillation.py \
    --distillation-type logit \
    --train-data data/distillation/train.jsonl \
    --student-config configs/model/100m_laptop.json \
    --teacher-model codellama/CodeLlama-7b-hf \
    --alpha 0.5 \
    --temperature 2.0 \
    --output-dir checkpoints/distilled_100m_soft
```

### 3. Hybrid Distillation (Best of Both)

**What it is:** Combines output distillation (hard labels) with logit distillation (soft labels)

**Loss function:**
```
Loss = α * CrossEntropy(student, labels) + (1 - α) * KL(student || teacher)
```

**Pros:**
- ✅ Best of both worlds
- ✅ More robust training
- ✅ Better generalization

**Cons:**
- ❌ Same cons as logit distillation
- ❌ One more hyperparameter (alpha)

**Best for:** Production models, when you have GPU resources

**Example:**
```bash
python scripts/train_with_distillation.py \
    --distillation-type hybrid \
    --train-data data/distillation/train.jsonl \
    --student-config configs/model/350m_laptop.json \
    --teacher-model codellama/CodeLlama-13b-hf \
    --alpha 0.3 \
    --temperature 2.5 \
    --output-dir checkpoints/distilled_350m_hybrid
```

## Quick Start

### Step 1: Prepare Data

Generate training data using GPT-4/Claude:

```bash
# Generate 10K examples (~$1000-1500)
python scripts/generate_distillation_data.py \
    --provider openai \
    --model gpt-4-turbo \
    --num-examples 10000 \
    --output-dir data/distillation
```

This creates `data/distillation/train.jsonl`.

### Step 2: Choose Your Approach

**Laptop/Limited Hardware:**
```bash
# Output distillation (100M model)
python scripts/train_with_distillation.py \
    --distillation-type output \
    --train-data data/distillation/train.jsonl \
    --student-config configs/model/100m_laptop.json \
    --batch-size 8 \
    --gradient-accumulation-steps 4 \
    --max-steps 50000 \
    --output-dir checkpoints/100m_output
```

**Cloud with Single GPU:**
```bash
# Logit distillation (100M student, CodeLlama-7B teacher)
python scripts/train_with_distillation.py \
    --distillation-type logit \
    --train-data data/distillation/train.jsonl \
    --student-config configs/model/100m_laptop.json \
    --teacher-model codellama/CodeLlama-7b-hf \
    --batch-size 4 \
    --gradient-accumulation-steps 8 \
    --alpha 0.5 \
    --temperature 2.0 \
    --max-steps 50000 \
    --use-wandb \
    --wandb-project demiurgic-distillation \
    --output-dir checkpoints/100m_logit
```

**Cloud with Multi-GPU:**
```bash
# Hybrid distillation (350M student, CodeLlama-13B teacher)
python scripts/train_with_distillation.py \
    --distillation-type hybrid \
    --train-data data/distillation/train.jsonl \
    --student-config configs/model/350m_laptop.json \
    --teacher-model codellama/CodeLlama-13b-hf \
    --batch-size 4 \
    --gradient-accumulation-steps 8 \
    --alpha 0.3 \
    --temperature 2.5 \
    --max-steps 100000 \
    --use-wandb \
    --wandb-project demiurgic-distillation \
    --output-dir checkpoints/350m_hybrid
```

## Hyperparameters

### Critical Parameters

**`--alpha`** (0.0 - 1.0, default: 0.5)
- Weight for hard loss (cross-entropy with labels)
- `1.0` = only hard loss (output distillation)
- `0.5` = balance hard and soft
- `0.3` = favor soft loss (more teacher influence)

**`--temperature`** (> 0, default: 2.0)
- Controls "softness" of probability distributions
- Higher = softer (smoother distributions)
- Typical range: 1.5 - 4.0
- 2.0 - 3.0 works well for most cases

**`--batch-size`** and **`--gradient-accumulation-steps`**
- Effective batch size = batch_size × gradient_accumulation_steps
- Larger effective batch = more stable but slower
- Recommended effective batch: 32-128

**`--learning-rate`** (default: 1e-4)
- Start with 1e-4, adjust based on loss
- Lower if training is unstable
- Higher for faster convergence (but risky)

### Training Length

**`--max-steps`**
- 50K steps for 100M model
- 100K steps for 350M model
- 200K+ steps for 1B+ models

**Rule of thumb:**
- Each example should be seen 2-4 times
- `steps = (num_examples × epochs) / effective_batch_size`

### Memory Optimization

**`--use-gradient-checkpointing`** (default: true)
- Saves memory at cost of ~20% slower training
- Essential for larger models

**`--use-mixed-precision`** (default: true)
- Use fp16/bf16 instead of fp32
- 2x memory reduction, faster training
- Use `bf16` if available (better for large models)

## Hardware Requirements

### Output Distillation

| Student Size | GPU Memory | Recommendation |
|--------------|------------|----------------|
| 100M | 4-6 GB | Laptop GPU OK |
| 350M | 8-12 GB | GTX 1080 Ti / RTX 2060+ |
| 1B | 16-24 GB | RTX 3090 / V100 |
| 7B | 40-48 GB | A100 40GB |

### Logit Distillation (Student + Teacher)

| Student | Teacher | GPU Memory | Recommendation |
|---------|---------|------------|----------------|
| 100M | CodeLlama-7B | 16-20 GB | RTX 3090 / V100 |
| 350M | CodeLlama-7B | 20-24 GB | RTX 4090 / V100 |
| 350M | CodeLlama-13B | 28-32 GB | A100 40GB |
| 1B | CodeLlama-7B | 32-40 GB | A100 40GB |

## Training Time Estimates

**Output Distillation (100M, 50K steps):**
- RTX 3090: 12-16 hours
- A100 40GB: 8-10 hours
- V100 32GB: 14-18 hours

**Logit Distillation (100M + CodeLlama-7B, 50K steps):**
- RTX 3090: 24-30 hours
- A100 40GB: 16-20 hours
- V100 32GB: 28-36 hours

**Hybrid Distillation (350M + CodeLlama-13B, 100K steps):**
- A100 40GB: 3-4 days
- A100 80GB: 2-3 days

## Monitoring Training

### Using W&B (Recommended)

```bash
# Add these flags
--use-wandb \
--wandb-project demiurgic-distillation \
--wandb-run-name my-experiment
```

Metrics tracked:
- `total_loss`: Combined distillation loss
- `hard_loss`: Cross-entropy with labels
- `soft_loss`: KL divergence with teacher
- `learning_rate`: Current LR
- `perplexity`: Model perplexity

### Command Line Output

```
Training: 100%|████████| 50000/50000 [12:34:56, total_loss=2.34, hard_loss=2.56, soft_loss=2.12]

Checkpoint saved to checkpoints/100m_output/checkpoint-1000
Checkpoint saved to checkpoints/100m_output/checkpoint-2000
...
```

## Checkpoints

### Automatic Checkpointing

Saves every `--save-steps` (default: 1000):

```
checkpoints/100m_output/
├── checkpoint-1000/
│   ├── config.json
│   ├── pytorch_model.bin
│   ├── tokenizer_config.json
│   └── training_state.pt
├── checkpoint-2000/
└── final/
    └── ...
```

### Resuming Training

```bash
python scripts/train_with_distillation.py \
    ... \
    --resume-from-checkpoint checkpoints/100m_output/checkpoint-10000
```

### Checkpoint Management

`--save-total-limit 3` keeps only last 3 checkpoints (saves disk space)

## Advanced Usage

### Custom Dataset Format

Your JSONL should have:
```json
{
  "prompt": "Write a function to...",
  "response": "```python\ndef ...",
  "category": "function_implementation",
  "language": "python"
}
```

### Multiple GPUs (Coming Soon)

```bash
# Distributed training
torchrun --nproc_per_node=4 scripts/train_with_distillation.py \
    ...
```

### Custom Teacher Models

Use any HuggingFace model:
```bash
--teacher-model microsoft/phi-2
--teacher-model mistralai/Mistral-7B-v0.1
--teacher-model WizardLM/WizardCoder-15B-V1.0
```

### Fine-tuning a Checkpoint

Continue training from a saved model:
```bash
# Load student from checkpoint instead of random init
--resume-from-checkpoint checkpoints/100m_output/checkpoint-50000
```

## Troubleshooting

### Out of Memory

**Solutions:**
1. Reduce batch size: `--batch-size 2`
2. Increase gradient accumulation: `--gradient-accumulation-steps 16`
3. Enable gradient checkpointing (already on by default)
4. Reduce sequence length: `--max-seq-length 1024`
5. For logit distillation, use smaller teacher model

### Loss Not Decreasing

**Checks:**
1. Is learning rate too high? Try `--learning-rate 5e-5`
2. Is data quality good? Check examples manually
3. Are labels correct? Verify dataset format
4. Try different temperature: `--temperature 1.5` or `3.0`

### Training Too Slow

**Optimizations:**
1. Use mixed precision (enabled by default)
2. Increase batch size if GPU memory allows
3. Use fewer workers: `--num-workers 2`
4. For logit distillation, use smaller teacher

### Soft Loss is NaN

**Cause:** Temperature too low or numerical instability

**Fix:**
```bash
--temperature 2.5  # Increase temperature
--mixed-precision-dtype bf16  # Use bfloat16 instead of fp16
```

## Next Steps

After training:

### 1. Evaluate Your Model

```bash
# Coming soon
python scripts/evaluate_model.py \
    --model checkpoints/100m_output/final \
    --benchmark humaneval
```

### 2. Test Generation

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("checkpoints/100m_output/final")
tokenizer = AutoTokenizer.from_pretrained("checkpoints/100m_output/final")

prompt = "Write a Python function to reverse a string:"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
```

### 3. Compare Models

Train with different approaches and compare:
- Output distillation baseline
- Logit distillation with different teachers
- Hybrid with different alpha/temperature

### 4. Iterate

- Identify weak areas
- Generate targeted distillation data
- Fine-tune on specific tasks

## Summary

**Quick Decision Guide:**

| Your Situation | Recommended Approach |
|----------------|---------------------|
| Laptop, CPU only | Output distillation, 100M model |
| Laptop, 8GB GPU | Output distillation, 100M model |
| Cloud, 16GB GPU | Logit distillation, 100M + CodeLlama-7B |
| Cloud, 24GB GPU | Logit distillation, 350M + CodeLlama-7B |
| Cloud, 40GB GPU | Hybrid, 350M + CodeLlama-13B |
| Cloud, 80GB GPU | Hybrid, 1B + CodeLlama-34B |

**Recommended First Run:**
```bash
# Generate data
python scripts/generate_distillation_data.py \
    --num-examples 1000 \
    --output-dir data/distillation/test

# Train (output distillation - works on laptop)
python scripts/train_with_distillation.py \
    --distillation-type output \
    --train-data data/distillation/test/train.jsonl \
    --student-config configs/model/100m_laptop.json \
    --max-steps 5000 \
    --output-dir checkpoints/test_100m

# Should take 1-2 hours on laptop
```

---

**Questions?** Check the code in `src/distillation/trainer.py` or open an issue.
